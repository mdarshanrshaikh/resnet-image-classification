{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TnVGxqKiVCiL"
      },
      "outputs": [],
      "source": [
        "#importing the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comfigurations\n",
        "IMG_SIZE = 32 #CIFAR-10 standard resolution for stratch training\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 1\n",
        "DATA_SAMPLE_SIZE = 5000\n",
        "TEST_SAMPLE_SIZE = 1000"
      ],
      "metadata": {
        "id": "5r6Ay86-Vcq_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the device to CPU\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "PNndRDhjV5Ul"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual block definition"
      ],
      "metadata": {
        "id": "uSQfVN6XWGJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  \"\"\"\n",
        "    The basic block for ResNet-18 and ResNet-34.\n",
        "    It contains two 3x3 convolutional layers.\n",
        "  \"\"\"\n",
        "  expansion = 1 #no channel expansion in BasicBlock\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "\n",
        "    #determine if the shortcut paths need a 1x1 convolution (downsampling/channel change)\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride!=1 or in_channels!=self.expansion*out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(self.expansion*out_channels)\n",
        "      )\n",
        "\n",
        "    #first convolutional layer (with potential stride for downsampling\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    #second convolutional layer\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(self.expansion*out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #the main F(x) function\n",
        "    out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "\n",
        "    #adding the shortcut path F(x)+x\n",
        "    out += self.shortcut(x)\n",
        "    out = nn.ReLU()(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "qm87EuI5V9jK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet model definition"
      ],
      "metadata": {
        "id": "kZOtPYFpYqLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "\n",
        "    #initial 3x3 convolution (optimized for CIFAR's 32x32 input)\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    #resnet stages (stacks of residual blocks)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) #downsampling\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) #downsampling\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) #downsampling\n",
        "\n",
        "    #using AdaptiveAvgPooling2D for robust global average pooling\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    #final layer\n",
        "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "    \"\"\"Creates a sequential layer of residual blocks.\"\"\"\n",
        "\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels * block.expansion\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "\n",
        "    #global average pooling (reducing HxW to 1x1)\n",
        "    out = self.avgpool(out) #changed to size 4 to handle 32x32 downsampling\n",
        "    out = out.view(out.size(0), -1)\n",
        "\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "BQ6nZu1pYo25"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18():\n",
        "  \"\"\"Factory function to build a ResNet-18 model.\"\"\"\n",
        "  #ResNet18 uses: [2 blocks, 2 blocks, 2 blocks, 2 blocks]\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "cCLM7lUlb3vh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading & preprocessing"
      ],
      "metadata": {
        "id": "K4YLl2UUcWhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10_datasets():\n",
        "    \"\"\"\n",
        "    Loads CIFAR-10, applies transformations, and subsamples the data.\n",
        "    \"\"\"\n",
        "    #Standard normalization for CIFAR-10 when training from scratch\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    #Loading full datasets\n",
        "    train_dataset_full = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset_full = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    #Subsampling the data to reduce training time ---\n",
        "    train_indices = np.arange(DATA_SAMPLE_SIZE)\n",
        "    train_dataset = Subset(train_dataset_full, train_indices)\n",
        "\n",
        "    test_indices = np.arange(TEST_SAMPLE_SIZE)\n",
        "    test_dataset = Subset(test_dataset_full, test_indices)\n",
        "\n",
        "    #Creating DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "zTjwcTpTcVru"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & evaluation functions"
      ],
      "metadata": {
        "id": "nq--1wLHcmJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader):\n",
        "    \"\"\"\n",
        "    Trains the model for the configured number of epochs.\n",
        "    \"\"\"\n",
        "    #Optimizing ALL parameters (since we are training from scratch)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                 print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "iHfQPRtXcjzQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on the test set.\n",
        "    \"\"\"\n",
        "    model.eval() # Setting model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # Disabling gradient calculation during evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels).item()\n",
        "\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "amaS70hycyQu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution"
      ],
      "metadata": {
        "id": "rSxvsjyFc9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading and prepare data (using 32x32 resolution)\n",
        "train_loader, test_loader = load_cifar10_datasets()"
      ],
      "metadata": {
        "id": "OrmE-ojpc63h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the ResNet-18 model\n",
        "model = ResNet18()"
      ],
      "metadata": {
        "id": "SLk2oc4HdAeb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure to the console\n",
        "print(model, file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YpsXXUCdFsm",
        "outputId": "9cbb7e31-e79e-4ddb-c82b-89a3832e173d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (shortcut): Sequential()\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (shortcut): Sequential()\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (shortcut): Sequential()\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (shortcut): Sequential()\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (shortcut): Sequential()\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and evaluating the model\n",
        "train_model(model, train_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egM7jARTdMU3",
        "outputId": "8a97e960-6122-440f-f7ed-3b28c7047ae5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 Loss: 2.4528\n",
            "Test Loss: 2.0301 (Last Batch)\n",
            "Test Accuracy: 24.00%\n"
          ]
        }
      ]
    }
  ]
}