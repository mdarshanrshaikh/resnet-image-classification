{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-f8ws2F59Ry"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNDuGrrb6gyH"
      },
      "outputs": [],
      "source": [
        "#configurations\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 1\n",
        "DATA_SAMPLE_SIZE = 5000\n",
        "TEST_SAMPLE_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6EM7vY16-Mh"
      },
      "outputs": [],
      "source": [
        "#setting device to CPU\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Bjqou_7KXU"
      },
      "source": [
        "Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfY1yYjO7FhZ"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_datasets():\n",
        "  \"\"\"\n",
        "    Loads CIFAR-10, applies transformations, and subsamples the data.\n",
        "  \"\"\"\n",
        "  #defining the necessary transformations for Resnet-50\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize(IMG_SIZE),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #standard normalization for ImageNet trained models\n",
        "  ])\n",
        "\n",
        "  #load datasets\n",
        "  train_dataset_full = datasets.CIFAR10(root='/data', train=True, download=True, transform=transform)\n",
        "  test_dataset_full = datasets.CIFAR10(root='/data', train=False, download=True, transform=transform)\n",
        "\n",
        "  #subsampling training data\n",
        "  train_indices = np.arange(DATA_SAMPLE_SIZE)\n",
        "  train_dataset = Subset(train_dataset_full, train_indices)\n",
        "\n",
        "  #subsampling testing data\n",
        "  test_indices = np.arange(TEST_SAMPLE_SIZE)\n",
        "  test_dataset = Subset(test_dataset_full, test_indices)\n",
        "\n",
        "  #create dataloaders\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFwolB-9oAc"
      },
      "source": [
        "Model definition & freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCDq7sme9l4R"
      },
      "outputs": [],
      "source": [
        "def build_transfer_model():\n",
        "  \"\"\"\n",
        "    Loads a pre-trained ResNet-50, freezes the backbone, and replaces the head.\n",
        "  \"\"\"\n",
        "  #loading the pre-trained ResNet-50 model\n",
        "  model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "  #freezing all the parameters in the pre-trained model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  #replacing the fully connected layer (classification head)\n",
        "  num_ftrs = model.fc.in_features #the original resnet50 models 50 features\n",
        "  #creating a new full connected layer\n",
        "  model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "\n",
        "  #moving the model to the device\n",
        "  model = model.to(device)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMoedXFY_jsa"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl2kaEud_fDG"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader):\n",
        "  \"\"\"\n",
        "    Trains the model for the configured number of epochs.\n",
        "  \"\"\"\n",
        "  #only optimizing the parameters in the new classification head\n",
        "  optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    model.train() #setting model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      #zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      #backward pass & optimization\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "      #printing loss every 100 batches for monitoring\n",
        "      if (i+1)%100 == 0:\n",
        "        print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss/len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "  #after training evaluate the final performance\n",
        "  evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkWJn8aqBzuu"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "  \"\"\"\n",
        "    Evaluates the model's performance on the test set.\n",
        "  \"\"\"\n",
        "  model.eval() #setting model to evaluation mode\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad(): #disabling gradient calculation during evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted==labels).sum().item()\n",
        "\n",
        "  accuracy = 100*correct/total\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels).item()\n",
        "\n",
        "  print(f\"Test Loss: {loss:.4f}\")\n",
        "  print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEzewvg4DPw8"
      },
      "source": [
        "Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeQnJIbMDMxo",
        "outputId": "c2ab90cd-61cd-4d11-f425-9229f5f068a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 40.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "#loading & preparing data\n",
        "train_loader, test_loader = load_cifar10_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xQIwlHoDc8W",
        "outputId": "94b494f9-6062-4bed-d6a7-feb47fe1db98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 122MB/s]\n"
          ]
        }
      ],
      "source": [
        "#build the model\n",
        "model = build_transfer_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WjUQbuYmDk5e",
        "outputId": "d6f22a5c-e0e3-4992-883f-0c87534008dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 Loss: 1.3722\n",
            "Test Loss: 0.7436\n",
            "Test Accuracy: 72.90%\n"
          ]
        }
      ],
      "source": [
        "#training & evaluating the model\n",
        "train_model(model, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_PATH = \"resnet18_transfer_learned.pth\""
      ],
      "metadata": {
        "id": "QSdbjKAQghtd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "Xg8Kh8sJgiBJ"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}